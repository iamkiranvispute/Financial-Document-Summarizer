{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Install dependencies (first-time setup)\n",
        "# ---------------------------\n",
        "\n",
        "#!pip install pdfplumber pandas tabulate transformers torch tqdm evaluate\n",
        "\n",
        "try:\n",
        "    import pdfplumber, pandas, re, tabulate, transformers, torch\n",
        "except ImportError:\n",
        "    import os\n",
        "    os.system(\"pip install pdfplumber pandas tabulate transformers torch\")\n",
        "    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "\n",
        "    %env CUDA_LAUNCH_BLOCKING=1"
      ],
      "metadata": {
        "id": "pZw0oua6efwT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Imports\n",
        "# ---------------------------\n",
        "\n",
        "import pdfplumber\n",
        "import pandas as pd\n",
        "import re\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "import tabulate\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "J8JjfJoLenwO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Setup device\n",
        "# ============================================================\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(\"Using device:\", \"GPU\" if device == 0 else \"CPU\")"
      ],
      "metadata": {
        "id": "b684o046msvJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c74629ab-6823-4990-ffe5-76f4c6e5963c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Tokenizer for safe truncation\n",
        "# ============================================================\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "def safe_encode(text, max_tokens=1024):\n",
        "    \"\"\"Ensure input fits within model limits.\"\"\"\n",
        "    if not text or not text.strip():\n",
        "        return None\n",
        "    tokens = tokenizer.encode(text, truncation=True, max_length=max_tokens)\n",
        "    return tokenizer.decode(tokens, skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "45sDiWkTms82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97940ef4-2406-472b-d4ea-d673100dad8f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Utility functions\n",
        "# ---------------------------\n",
        "\n",
        "def clean_text(txt: str) -> str:\n",
        "    txt = str(txt)\n",
        "    txt = re.sub(r'\\n+', ' ', txt)              # remove newlines\n",
        "    txt = re.sub(r'\\s+', ' ', txt)              # normalize spaces\n",
        "    txt = re.sub(r'Page \\d+ of \\d+', '', txt)   # remove page numbers\n",
        "    txt = re.sub(r'(Integrated Annual Report \\d{4}-\\d{2})', '', txt)  # headers\n",
        "    txt = re.sub(r'-+\\s*$', '', txt)            # footers\n",
        "    return txt.strip()"
      ],
      "metadata": {
        "id": "oPwCpSOmeri9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_columns(page):\n",
        "    \"\"\"Handle 2-column layouts, fallback to single column if empty.\"\"\"\n",
        "    width = page.width\n",
        "    left_bbox = (0, 0, width / 2, page.height)\n",
        "    right_bbox = (width / 2, 0, width, page.height)\n",
        "\n",
        "    left_text = page.within_bbox(left_bbox).extract_text()\n",
        "    right_text = page.within_bbox(right_bbox).extract_text()\n",
        "\n",
        "    if not left_text and not right_text:\n",
        "        return page.extract_text() or \"\"\n",
        "    return (left_text or \"\") + \"\\n\" + (right_text or \"\")"
      ],
      "metadata": {
        "id": "YvLsDnRleuHW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_tables(pdf_path):\n",
        "    \"\"\"Extract tables from PDF and convert to pandas DataFrames.\"\"\"\n",
        "    tables = []\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            page_tables = page.extract_tables()\n",
        "            for table in page_tables:\n",
        "                if table:\n",
        "                    # first row is header\n",
        "                    df = pd.DataFrame(table[1:], columns=table[0])\n",
        "                    tables.append(df)\n",
        "    return tables"
      ],
      "metadata": {
        "id": "V8FbzU8gewu4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_pdf(pdf_path):\n",
        "    texts = []\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            col_text = extract_columns(page)\n",
        "            col_text = clean_text(col_text)\n",
        "            if len(col_text) > 100:  # skip very short/empty\n",
        "                texts.append(col_text)\n",
        "    return texts"
      ],
      "metadata": {
        "id": "K3yXiZeiey-q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_tables(tables):\n",
        "    \"\"\"Convert tables to markdown-like text for summarization.\"\"\"\n",
        "    tables_text = []\n",
        "    for df in tables:\n",
        "        tab_txt = tabulate.tabulate(df, headers=\"keys\", tablefmt=\"pipe\")\n",
        "        tables_text.append(tab_txt)\n",
        "    return tables_text"
      ],
      "metadata": {
        "id": "MG22-lSTe1wD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_texts(texts, chunk_size=3000):\n",
        "    \"\"\"Split long texts into smaller chunks.\"\"\"\n",
        "    chunks = []\n",
        "    for txt in texts:\n",
        "        for i in range(0, len(txt), chunk_size):\n",
        "            chunk = txt[i:i+chunk_size]\n",
        "            if len(chunk) > 50:\n",
        "                chunks.append(chunk)\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "uE85CD_re3zg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 6. Safe summarization function (fixes CUDA crash)\n",
        "# ============================================================\n",
        "def summarize_with_safe_params(text, summarizer, default_max=120, default_min=20):\n",
        "    safe_text = safe_encode(text, max_tokens=1024)\n",
        "    if not safe_text:\n",
        "        return \"\"  # skip empty\n",
        "\n",
        "    input_len = len(safe_text.split())\n",
        "\n",
        "    # dynamic max_length\n",
        "    if input_len < 50:\n",
        "        max_len = max(15, int(input_len * 0.6))\n",
        "    elif input_len < 200:\n",
        "        max_len = max(30, int(input_len * 0.7))\n",
        "    else:\n",
        "        max_len = default_max\n",
        "\n",
        "    # ensure min_length < max_length\n",
        "    min_len = max(5, min(default_min, max_len - 5))\n",
        "\n",
        "    return summarizer(\n",
        "        safe_text,\n",
        "        max_length=max_len,\n",
        "        min_length=min_len,\n",
        "        truncation=True\n",
        "    )[0][\"summary_text\"]\n",
        "\n",
        "def batch_summarize(chunks, summarizer):\n",
        "    summaries = []\n",
        "    for i in tqdm(range(len(chunks))):\n",
        "        summaries.append(summarize_with_safe_params(chunks[i], summarizer))\n",
        "    return summaries\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 7. Improved Table Summarization\n",
        "# ============================================================\n",
        "def narrate_table(df, max_rows=5):\n",
        "    \"\"\"Convert a dataframe into natural language narration for first few rows.\"\"\"\n",
        "    narrations = []\n",
        "    for i, row in df.head(max_rows).iterrows():\n",
        "        parts = []\n",
        "        for col in df.columns:\n",
        "            val = str(row[col]).strip()\n",
        "            if val and val.lower() != \"nan\":\n",
        "                parts.append(f\"{col}: {val}\")\n",
        "        if parts:\n",
        "            narrations.append(f\"Row {i+1}: \" + \", \".join(parts))\n",
        "    return \" \".join(narrations)\n",
        "\n",
        "def table_descriptive_summary(df):\n",
        "    \"\"\"Generate simple numeric stats for numeric columns.\"\"\"\n",
        "    summary_lines = []\n",
        "    for col in df.columns:\n",
        "        try:\n",
        "            numeric = pd.to_numeric(df[col], errors=\"coerce\").dropna()\n",
        "            if not numeric.empty:\n",
        "                summary_lines.append(\n",
        "                    f\"For column '{col}', min={numeric.min()}, max={numeric.max()}, mean={numeric.mean():.2f}.\"\n",
        "                )\n",
        "        except Exception:\n",
        "            continue\n",
        "    return \" \".join(summary_lines)\n",
        "\n",
        "def summarize_tables(tables, summarizer):\n",
        "    \"\"\"Summarize tables by converting to narration + numeric stats before BART.\"\"\"\n",
        "    summaries = []\n",
        "    for df in tables:\n",
        "        narr_text = narrate_table(df, max_rows=5)\n",
        "        stats_text = table_descriptive_summary(df)\n",
        "        combined_text = (narr_text + \" \" + stats_text).strip()\n",
        "\n",
        "        if not combined_text:\n",
        "            summaries.append(\"Table skipped (empty or invalid).\")\n",
        "            continue\n",
        "\n",
        "        summary = summarize_with_safe_params(\n",
        "            combined_text,\n",
        "            summarizer,\n",
        "            default_max=120,\n",
        "            default_min=30\n",
        "        )\n",
        "        summaries.append(summary)\n",
        "    return summaries\n"
      ],
      "metadata": {
        "id": "y4HK-PS8e5nU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 7. Multi-stage summarization\n",
        "# ============================================================\n",
        "def multi_stage_summary(chunk_summaries, summarizer, group_size=20):\n",
        "    # Stage 1: Group ~20 chunk summaries → section summaries\n",
        "    groups = []\n",
        "    for i in range(0, len(chunk_summaries), group_size):\n",
        "        groups.append(\" \".join(chunk_summaries[i:i+group_size]))\n",
        "\n",
        "    section_summaries = []\n",
        "    for g in groups:\n",
        "        res = summarize_with_safe_params(g, summarizer, default_max=400, default_min=150)\n",
        "        section_summaries.append(res)\n",
        "\n",
        "    # Stage 2: Merge all section summaries into final doc\n",
        "    final_summary = \" \".join(section_summaries)\n",
        "    return final_summary, section_summaries\n"
      ],
      "metadata": {
        "id": "Pa7y47nl8EgE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Main execution\n",
        "# ---------------------------\n",
        "\n",
        "pdf_path = \"/content/Annual report HDFC.pdf\"  # replace with your actual file path\n",
        "\n",
        "print(\"Extracting tables...\")\n",
        "tables = extract_tables(pdf_path)\n",
        "\n",
        "print(\"Extracting text...\")\n",
        "texts = preprocess_pdf(pdf_path)\n",
        "\n",
        "print(\"Loading summarizer model...\")\n",
        "summarizer = pipeline(\n",
        "    \"summarization\",\n",
        "    #model=\"facebook/bart-large-cnn\"\n",
        "    model=\"sshleifer/distilbart-cnn-12-6\"\n",
        "    #model=\"google/pegasus-xsum\"\n",
        "    #model=\"google/long-t5-local-base\"\n",
        "    ,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(\"Chunk-level summaries...\")\n",
        "chunks = chunk_texts(texts, 1500)\n",
        "chunk_summaries = batch_summarize(chunks, summarizer)\n",
        "\n",
        "print(\"Table summaries...\")\n",
        "#table_txts = preprocess_tables(tables)\n",
        "table_summaries = summarize_tables(tables, summarizer)\n",
        "\n",
        "print(\"Multi-stage final summary...\")\n",
        "final_summary_long, section_summaries = multi_stage_summary(chunk_summaries, summarizer)\n",
        "\n",
        "# ============================================================\n",
        "# 9. Save outputs\n",
        "# ============================================================\n",
        "pd.DataFrame({\"chunk_summary\": chunk_summaries}).to_csv(\"chunk_summaries_bart.csv\", index=False)\n",
        "pd.DataFrame({\"table_summary\": table_summaries}).to_csv(\"table_summaries_bart.csv\", index=False)\n",
        "pd.DataFrame({\"section_summary\": section_summaries}).to_csv(\"section_summaries.csv\", index=False)\n",
        "\n",
        "with open(\"final_summary_long.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(final_summary_long)\n",
        "\n",
        "print(\"✅ Outputs saved: chunk_summaries_bart.csv, table_summaries_bart.csv, section_summaries.csv, final_summary_long.txt\")"
      ],
      "metadata": {
        "id": "xQMoZossfKlQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b657a59a-b304-405b-d7b6-f7ea9f3e67ec"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting tables...\n",
            "Extracting text...\n",
            "Loading summarizer model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk-level summaries...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 10/1381 [00:07<17:11,  1.33it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            " 10%|█         | 144/1381 [01:22<12:39,  1.63it/s]Your max_length is set to 15, but your input_length is only 13. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=6)\n",
            " 14%|█▎        | 188/1381 [01:45<12:13,  1.63it/s]Your max_length is set to 15, but your input_length is only 10. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=5)\n",
            " 15%|█▌        | 214/1381 [01:59<09:58,  1.95it/s]Your max_length is set to 15, but your input_length is only 14. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=7)\n",
            " 36%|███▌      | 497/1381 [04:31<09:09,  1.61it/s]Your max_length is set to 15, but your input_length is only 10. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=5)\n",
            " 73%|███████▎  | 1009/1381 [09:44<03:20,  1.86it/s]Your max_length is set to 15, but your input_length is only 14. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=7)\n",
            "100%|██████████| 1381/1381 [13:26<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table summaries...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 15, but your input_length is only 13. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=6)\n",
            "Your max_length is set to 15, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
            "Your max_length is set to 15, but your input_length is only 13. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=6)\n",
            "Your max_length is set to 15, but your input_length is only 13. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=6)\n",
            "Your max_length is set to 15, but your input_length is only 11. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multi-stage final summary...\n",
            "✅ Outputs saved: chunk_summaries_bart.csv, table_summaries_bart.csv, section_summaries.csv, final_summary_long.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Evaluation\n",
        "# ============================================================\n",
        "def compression_ratio(original_texts, summaries):\n",
        "    orig_len = sum(len(t.split()) for t in original_texts)\n",
        "    sum_len = sum(len(s.split()) for s in summaries)\n",
        "    return round(sum_len / orig_len, 3) if orig_len > 0 else 0\n",
        "\n",
        "def avg_summary_length(summaries):\n",
        "    return round(sum(len(s.split()) for s in summaries) / len(summaries), 2)\n",
        "\n",
        "print(\"\\n--- Evaluation Results ---\")\n",
        "print(\"Compression ratio:\", compression_ratio(texts, chunk_summaries))\n",
        "print(\"Avg chunk summary length:\", avg_summary_length(chunk_summaries))"
      ],
      "metadata": {
        "id": "_zhbtK_WX-ln",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ebbc7a0-550e-4217-d399-dec43fd42729"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluation Results ---\n",
            "Compression ratio: 0.215\n",
            "Avg chunk summary length: 40.15\n"
          ]
        }
      ]
    }
  ]
}